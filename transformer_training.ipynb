{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QdLo9cxIYtSWlnDsTYo6NccHxD_NbNt1",
      "authorship_tag": "ABX9TyNSEtBzWFqotQR3TmClaj0X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonEllershaw/latent_variable_OOD/blob/main/transformer_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvBiYfvE6O1G",
        "outputId": "16b785d7-7e51-4c2d-eda6-7574f69308fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pip transformers torch\n",
        "# ! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir ~/.kaggle\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "v1HaE2L98QYs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle datasets download tboyle10/medicaltranscriptions -p data/\n",
        "# ! unzip data/clinc150_uci.zip -d data"
      ],
      "metadata": {
        "id": "L_m6ZPt-9ZD1"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/data/clinc150_uci/data_full.json') as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "2KCl3NUu8Ovl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcxfnd8N9BSh",
        "outputId": "54816899-9833-49e3-e0bc-f11420456830"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['oos_val', 'val', 'train', 'oos_test', 'test', 'oos_train'])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['oos_train'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVgCH5yjPBGk",
        "outputId": "5ef3975f-002d-449b-83f4-3cbada5f82fd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['how much is an overdraft fee for bank', 'oos'],\n",
              " ['why are exponents preformed before multiplication in the order of operations',\n",
              "  'oos'],\n",
              " ['what size wipers does this car take', 'oos'],\n",
              " ['where is the dipstick', 'oos'],\n",
              " ['how much is 1 share of aapl', 'oos'],\n",
              " ['how is glue made', 'oos'],\n",
              " ['any headlines from my area', 'oos'],\n",
              " ['what is the largest state in the us', 'oos'],\n",
              " ['what is the current market trend', 'oos'],\n",
              " ['what is the most popular airline', 'oos']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ood_keys = [key for key in data.keys() if 'oos' in key]\n",
        "data['oos'] = []\n",
        "for key in ood_keys:\n",
        "  data['oos'] += data.pop(key)"
      ],
      "metadata": {
        "id": "9QrzPiSzOOPv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmjNobSU967f",
        "outputId": "4ec51615-d552-43f9-952d-11822282dbce"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['what expression would i use to say i love you if i were an italian',\n",
              "  'translate'],\n",
              " [\"can you tell me how to say 'i do not speak much spanish', in spanish\",\n",
              "  'translate'],\n",
              " [\"what is the equivalent of, 'life is good' in french\", 'translate'],\n",
              " [\"tell me how to say, 'it is a beautiful morning' in italian\", 'translate'],\n",
              " ['if i were mongolian, how would i say that i am a tourist', 'translate'],\n",
              " [\"how do i say 'hotel' in finnish\", 'translate'],\n",
              " [\"i need you to translate the sentence, 'we will be there soon' into portuguese\",\n",
              "  'translate'],\n",
              " ['please tell me how to ask for a taxi in french', 'translate'],\n",
              " [\"can you tell me how i would say, 'more bread please' in french\",\n",
              "  'translate'],\n",
              " [\"what is the correct way to say 'i am a visitor' in french\", 'translate']]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['oos'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvdke6g4O1IR",
        "outputId": "29e3f295-f41a-42d3-985e-35176e2951a9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['set a warning for when my bank account starts running low', 'oos'],\n",
              " ['a show on broadway', 'oos'],\n",
              " ['who has the best record in the nfl', 'oos'],\n",
              " ['how do i find the area of a circle', 'oos'],\n",
              " ['how many onions do i have on hand', 'oos'],\n",
              " ['what is the name of the 13th president', 'oos'],\n",
              " ['show me recent activity in my backyard', 'oos'],\n",
              " ['how long will it take me to pay off my card if i pay an extra $50 a month over the minimum',\n",
              "  'oos'],\n",
              " ['does our bank have free notary', 'oos'],\n",
              " ['what were the top stories this week', 'oos']]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "checkpoint = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ],
      "metadata": {
        "id": "0HRmKobDILmH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HuggingFaceDataset(Dataset):\n",
        "  def __init__(self, label2id, tokenizer, texts, labels=None):\n",
        "    self._dict = tokenizer(texts, truncation=True)\n",
        "    if labels is not None:\n",
        "      self._dict['labels'] = [label2id[label] for label in labels]\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self._dict[\"input_ids\"])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return {k: v[idx] for k, v in self._dict.items()}\n",
        "\n",
        "label_names = sorted({label for _, label in data['train']})\n",
        "id2label = dict(enumerate(sorted(label_names)))\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "datasets_hf = {}\n",
        "for key, dataset in data.items():\n",
        "  texts, labels = map(list,zip(*dataset))\n",
        "  if labels[0] == 'oos':\n",
        "    labels = None\n",
        "  datasets_hf[key] = HuggingFaceDataset(label2id, tokenizer, texts, labels) "
      ],
      "metadata": {
        "id": "-lmnAdNNEjqi"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, dataset in datasets_hf.items():\n",
        "  print(key, len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvZ2D0kpQVYk",
        "outputId": "8c85f577-643f-4618-9262-1235b96da335"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val 3000\n",
            "train 15000\n",
            "test 4500\n",
            "oos 1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(sample['input_ids']) for sample in datasets_hf['train']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "ueVQCkDht6Yk",
        "outputId": "51f19497-dc89-4b51-8db5-91ab81a20d56"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.013e+03, 4.410e+03, 6.366e+03, 2.022e+03, 7.220e+02, 3.610e+02,\n",
              "        6.800e+01, 3.200e+01, 4.000e+00, 2.000e+00]),\n",
              " array([ 3. ,  6.4,  9.8, 13.2, 16.6, 20. , 23.4, 26.8, 30.2, 33.6, 37. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no8WjPFcRsKd",
        "outputId": "ea23bb64-b4c2-45da-dc40-7d5371202b63"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size=32\n",
        "args = TrainingArguments(\n",
        "    \"inscope_text_classification\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    group_by_length=True,\n",
        "    warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "w1tvUeaNR4eo"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  \"\"\"Called to compute validation metrics at each evaluation step of training.\"\"\"\n",
        "  logits, labels = eval_pred\n",
        "  labels = labels.flatten()\n",
        "  predictions = np.argmax(logits, axis=-1).flatten()\n",
        "\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "      labels, predictions, average='macro', zero_division=0\n",
        "  )\n",
        "  metrics = {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "N37ouBZxS4hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasets_hf['train'],\n",
        "    eval_dataset=datasets_hf['val'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IStnRUjESgUZ",
        "outputId": "6d3c98b4-dcc0-415d-a2af-6bdda3cb9ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 15000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4690\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon_ellershaw\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/simon_ellershaw/huggingface/runs/17kveljx\" target=\"_blank\">inscope_text_classification</a></strong> to <a href=\"https://wandb.ai/simon_ellershaw/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4690' max='4690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4690/4690 10:59, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.507900</td>\n",
              "      <td>1.038306</td>\n",
              "      <td>0.903988</td>\n",
              "      <td>0.915600</td>\n",
              "      <td>0.907333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.469100</td>\n",
              "      <td>0.221534</td>\n",
              "      <td>0.962404</td>\n",
              "      <td>0.965474</td>\n",
              "      <td>0.962667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.108800</td>\n",
              "      <td>0.170812</td>\n",
              "      <td>0.965189</td>\n",
              "      <td>0.967871</td>\n",
              "      <td>0.965333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.182443</td>\n",
              "      <td>0.966577</td>\n",
              "      <td>0.969988</td>\n",
              "      <td>0.966333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.033600</td>\n",
              "      <td>0.170132</td>\n",
              "      <td>0.971933</td>\n",
              "      <td>0.973894</td>\n",
              "      <td>0.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.020400</td>\n",
              "      <td>0.163326</td>\n",
              "      <td>0.972200</td>\n",
              "      <td>0.974271</td>\n",
              "      <td>0.972333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.160175</td>\n",
              "      <td>0.973853</td>\n",
              "      <td>0.975546</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.008700</td>\n",
              "      <td>0.167610</td>\n",
              "      <td>0.975209</td>\n",
              "      <td>0.977163</td>\n",
              "      <td>0.975333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>0.166190</td>\n",
              "      <td>0.975937</td>\n",
              "      <td>0.977739</td>\n",
              "      <td>0.976000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.167297</td>\n",
              "      <td>0.975598</td>\n",
              "      <td>0.977366</td>\n",
              "      <td>0.975667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-469\n",
            "Configuration saved in inscope_text_classification/checkpoint-469/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-469/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-938\n",
            "Configuration saved in inscope_text_classification/checkpoint-938/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-938/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-1407\n",
            "Configuration saved in inscope_text_classification/checkpoint-1407/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-1407/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-1876\n",
            "Configuration saved in inscope_text_classification/checkpoint-1876/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-1876/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-2345\n",
            "Configuration saved in inscope_text_classification/checkpoint-2345/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-2345/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-2814\n",
            "Configuration saved in inscope_text_classification/checkpoint-2814/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-2814/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-3283\n",
            "Configuration saved in inscope_text_classification/checkpoint-3283/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-3283/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-3752\n",
            "Configuration saved in inscope_text_classification/checkpoint-3752/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-3752/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-4221\n",
            "Configuration saved in inscope_text_classification/checkpoint-4221/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-4221/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to inscope_text_classification/checkpoint-4690\n",
            "Configuration saved in inscope_text_classification/checkpoint-4690/config.json\n",
            "Model weights saved in inscope_text_classification/checkpoint-4690/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from inscope_text_classification/checkpoint-3283 (score: 0.16017483174800873).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4690, training_loss=0.42247336266645746, metrics={'train_runtime': 664.0677, 'train_samples_per_second': 225.881, 'train_steps_per_second': 7.063, 'total_flos': 883268415517824.0, 'train_loss': 0.42247336266645746, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('/content/inscope_text_classification/checkpoint-3283')"
      ],
      "metadata": {
        "id": "G-kch_koQ5LW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "model.eval()\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets_hf['train'], batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "outputs = []\n",
        "with torch.set_grad_enabled(False):\n",
        "    for batch in dataloader:\n",
        "        batch.pop(\"labels\", default=None)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs.append(model(**batch, output_hidden_states=True))"
      ],
      "metadata": {
        "id": "bqe12pbVRGIB"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLdWMgQeSZwi",
        "outputId": "de6b0a76-ddc9-49b7-d2a6-9b32aaaa4536"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"_name_or_path\": \"/content/inscope_text_classification/checkpoint-3283\",\n",
              "  \"architectures\": [\n",
              "    \"RobertaForSequenceClassification\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"accept_reservations\",\n",
              "    \"1\": \"account_blocked\",\n",
              "    \"2\": \"alarm\",\n",
              "    \"3\": \"application_status\",\n",
              "    \"4\": \"apr\",\n",
              "    \"5\": \"are_you_a_bot\",\n",
              "    \"6\": \"balance\",\n",
              "    \"7\": \"bill_balance\",\n",
              "    \"8\": \"bill_due\",\n",
              "    \"9\": \"book_flight\",\n",
              "    \"10\": \"book_hotel\",\n",
              "    \"11\": \"calculator\",\n",
              "    \"12\": \"calendar\",\n",
              "    \"13\": \"calendar_update\",\n",
              "    \"14\": \"calories\",\n",
              "    \"15\": \"cancel\",\n",
              "    \"16\": \"cancel_reservation\",\n",
              "    \"17\": \"car_rental\",\n",
              "    \"18\": \"card_declined\",\n",
              "    \"19\": \"carry_on\",\n",
              "    \"20\": \"change_accent\",\n",
              "    \"21\": \"change_ai_name\",\n",
              "    \"22\": \"change_language\",\n",
              "    \"23\": \"change_speed\",\n",
              "    \"24\": \"change_user_name\",\n",
              "    \"25\": \"change_volume\",\n",
              "    \"26\": \"confirm_reservation\",\n",
              "    \"27\": \"cook_time\",\n",
              "    \"28\": \"credit_limit\",\n",
              "    \"29\": \"credit_limit_change\",\n",
              "    \"30\": \"credit_score\",\n",
              "    \"31\": \"current_location\",\n",
              "    \"32\": \"damaged_card\",\n",
              "    \"33\": \"date\",\n",
              "    \"34\": \"definition\",\n",
              "    \"35\": \"direct_deposit\",\n",
              "    \"36\": \"directions\",\n",
              "    \"37\": \"distance\",\n",
              "    \"38\": \"do_you_have_pets\",\n",
              "    \"39\": \"exchange_rate\",\n",
              "    \"40\": \"expiration_date\",\n",
              "    \"41\": \"find_phone\",\n",
              "    \"42\": \"flight_status\",\n",
              "    \"43\": \"flip_coin\",\n",
              "    \"44\": \"food_last\",\n",
              "    \"45\": \"freeze_account\",\n",
              "    \"46\": \"fun_fact\",\n",
              "    \"47\": \"gas\",\n",
              "    \"48\": \"gas_type\",\n",
              "    \"49\": \"goodbye\",\n",
              "    \"50\": \"greeting\",\n",
              "    \"51\": \"how_busy\",\n",
              "    \"52\": \"how_old_are_you\",\n",
              "    \"53\": \"improve_credit_score\",\n",
              "    \"54\": \"income\",\n",
              "    \"55\": \"ingredient_substitution\",\n",
              "    \"56\": \"ingredients_list\",\n",
              "    \"57\": \"insurance\",\n",
              "    \"58\": \"insurance_change\",\n",
              "    \"59\": \"interest_rate\",\n",
              "    \"60\": \"international_fees\",\n",
              "    \"61\": \"international_visa\",\n",
              "    \"62\": \"jump_start\",\n",
              "    \"63\": \"last_maintenance\",\n",
              "    \"64\": \"lost_luggage\",\n",
              "    \"65\": \"make_call\",\n",
              "    \"66\": \"maybe\",\n",
              "    \"67\": \"meal_suggestion\",\n",
              "    \"68\": \"meaning_of_life\",\n",
              "    \"69\": \"measurement_conversion\",\n",
              "    \"70\": \"meeting_schedule\",\n",
              "    \"71\": \"min_payment\",\n",
              "    \"72\": \"mpg\",\n",
              "    \"73\": \"new_card\",\n",
              "    \"74\": \"next_holiday\",\n",
              "    \"75\": \"next_song\",\n",
              "    \"76\": \"no\",\n",
              "    \"77\": \"nutrition_info\",\n",
              "    \"78\": \"oil_change_how\",\n",
              "    \"79\": \"oil_change_when\",\n",
              "    \"80\": \"order\",\n",
              "    \"81\": \"order_checks\",\n",
              "    \"82\": \"order_status\",\n",
              "    \"83\": \"pay_bill\",\n",
              "    \"84\": \"payday\",\n",
              "    \"85\": \"pin_change\",\n",
              "    \"86\": \"play_music\",\n",
              "    \"87\": \"plug_type\",\n",
              "    \"88\": \"pto_balance\",\n",
              "    \"89\": \"pto_request\",\n",
              "    \"90\": \"pto_request_status\",\n",
              "    \"91\": \"pto_used\",\n",
              "    \"92\": \"recipe\",\n",
              "    \"93\": \"redeem_rewards\",\n",
              "    \"94\": \"reminder\",\n",
              "    \"95\": \"reminder_update\",\n",
              "    \"96\": \"repeat\",\n",
              "    \"97\": \"replacement_card_duration\",\n",
              "    \"98\": \"report_fraud\",\n",
              "    \"99\": \"report_lost_card\",\n",
              "    \"100\": \"reset_settings\",\n",
              "    \"101\": \"restaurant_reservation\",\n",
              "    \"102\": \"restaurant_reviews\",\n",
              "    \"103\": \"restaurant_suggestion\",\n",
              "    \"104\": \"rewards_balance\",\n",
              "    \"105\": \"roll_dice\",\n",
              "    \"106\": \"rollover_401k\",\n",
              "    \"107\": \"routing\",\n",
              "    \"108\": \"schedule_maintenance\",\n",
              "    \"109\": \"schedule_meeting\",\n",
              "    \"110\": \"share_location\",\n",
              "    \"111\": \"shopping_list\",\n",
              "    \"112\": \"shopping_list_update\",\n",
              "    \"113\": \"smart_home\",\n",
              "    \"114\": \"spelling\",\n",
              "    \"115\": \"spending_history\",\n",
              "    \"116\": \"sync_device\",\n",
              "    \"117\": \"taxes\",\n",
              "    \"118\": \"tell_joke\",\n",
              "    \"119\": \"text\",\n",
              "    \"120\": \"thank_you\",\n",
              "    \"121\": \"time\",\n",
              "    \"122\": \"timer\",\n",
              "    \"123\": \"timezone\",\n",
              "    \"124\": \"tire_change\",\n",
              "    \"125\": \"tire_pressure\",\n",
              "    \"126\": \"todo_list\",\n",
              "    \"127\": \"todo_list_update\",\n",
              "    \"128\": \"traffic\",\n",
              "    \"129\": \"transactions\",\n",
              "    \"130\": \"transfer\",\n",
              "    \"131\": \"translate\",\n",
              "    \"132\": \"travel_alert\",\n",
              "    \"133\": \"travel_notification\",\n",
              "    \"134\": \"travel_suggestion\",\n",
              "    \"135\": \"uber\",\n",
              "    \"136\": \"update_playlist\",\n",
              "    \"137\": \"user_name\",\n",
              "    \"138\": \"vaccines\",\n",
              "    \"139\": \"w2\",\n",
              "    \"140\": \"weather\",\n",
              "    \"141\": \"what_are_your_hobbies\",\n",
              "    \"142\": \"what_can_i_ask_you\",\n",
              "    \"143\": \"what_is_your_name\",\n",
              "    \"144\": \"what_song\",\n",
              "    \"145\": \"where_are_you_from\",\n",
              "    \"146\": \"whisper_mode\",\n",
              "    \"147\": \"who_do_you_work_for\",\n",
              "    \"148\": \"who_made_you\",\n",
              "    \"149\": \"yes\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"accept_reservations\": 0,\n",
              "    \"account_blocked\": 1,\n",
              "    \"alarm\": 2,\n",
              "    \"application_status\": 3,\n",
              "    \"apr\": 4,\n",
              "    \"are_you_a_bot\": 5,\n",
              "    \"balance\": 6,\n",
              "    \"bill_balance\": 7,\n",
              "    \"bill_due\": 8,\n",
              "    \"book_flight\": 9,\n",
              "    \"book_hotel\": 10,\n",
              "    \"calculator\": 11,\n",
              "    \"calendar\": 12,\n",
              "    \"calendar_update\": 13,\n",
              "    \"calories\": 14,\n",
              "    \"cancel\": 15,\n",
              "    \"cancel_reservation\": 16,\n",
              "    \"car_rental\": 17,\n",
              "    \"card_declined\": 18,\n",
              "    \"carry_on\": 19,\n",
              "    \"change_accent\": 20,\n",
              "    \"change_ai_name\": 21,\n",
              "    \"change_language\": 22,\n",
              "    \"change_speed\": 23,\n",
              "    \"change_user_name\": 24,\n",
              "    \"change_volume\": 25,\n",
              "    \"confirm_reservation\": 26,\n",
              "    \"cook_time\": 27,\n",
              "    \"credit_limit\": 28,\n",
              "    \"credit_limit_change\": 29,\n",
              "    \"credit_score\": 30,\n",
              "    \"current_location\": 31,\n",
              "    \"damaged_card\": 32,\n",
              "    \"date\": 33,\n",
              "    \"definition\": 34,\n",
              "    \"direct_deposit\": 35,\n",
              "    \"directions\": 36,\n",
              "    \"distance\": 37,\n",
              "    \"do_you_have_pets\": 38,\n",
              "    \"exchange_rate\": 39,\n",
              "    \"expiration_date\": 40,\n",
              "    \"find_phone\": 41,\n",
              "    \"flight_status\": 42,\n",
              "    \"flip_coin\": 43,\n",
              "    \"food_last\": 44,\n",
              "    \"freeze_account\": 45,\n",
              "    \"fun_fact\": 46,\n",
              "    \"gas\": 47,\n",
              "    \"gas_type\": 48,\n",
              "    \"goodbye\": 49,\n",
              "    \"greeting\": 50,\n",
              "    \"how_busy\": 51,\n",
              "    \"how_old_are_you\": 52,\n",
              "    \"improve_credit_score\": 53,\n",
              "    \"income\": 54,\n",
              "    \"ingredient_substitution\": 55,\n",
              "    \"ingredients_list\": 56,\n",
              "    \"insurance\": 57,\n",
              "    \"insurance_change\": 58,\n",
              "    \"interest_rate\": 59,\n",
              "    \"international_fees\": 60,\n",
              "    \"international_visa\": 61,\n",
              "    \"jump_start\": 62,\n",
              "    \"last_maintenance\": 63,\n",
              "    \"lost_luggage\": 64,\n",
              "    \"make_call\": 65,\n",
              "    \"maybe\": 66,\n",
              "    \"meal_suggestion\": 67,\n",
              "    \"meaning_of_life\": 68,\n",
              "    \"measurement_conversion\": 69,\n",
              "    \"meeting_schedule\": 70,\n",
              "    \"min_payment\": 71,\n",
              "    \"mpg\": 72,\n",
              "    \"new_card\": 73,\n",
              "    \"next_holiday\": 74,\n",
              "    \"next_song\": 75,\n",
              "    \"no\": 76,\n",
              "    \"nutrition_info\": 77,\n",
              "    \"oil_change_how\": 78,\n",
              "    \"oil_change_when\": 79,\n",
              "    \"order\": 80,\n",
              "    \"order_checks\": 81,\n",
              "    \"order_status\": 82,\n",
              "    \"pay_bill\": 83,\n",
              "    \"payday\": 84,\n",
              "    \"pin_change\": 85,\n",
              "    \"play_music\": 86,\n",
              "    \"plug_type\": 87,\n",
              "    \"pto_balance\": 88,\n",
              "    \"pto_request\": 89,\n",
              "    \"pto_request_status\": 90,\n",
              "    \"pto_used\": 91,\n",
              "    \"recipe\": 92,\n",
              "    \"redeem_rewards\": 93,\n",
              "    \"reminder\": 94,\n",
              "    \"reminder_update\": 95,\n",
              "    \"repeat\": 96,\n",
              "    \"replacement_card_duration\": 97,\n",
              "    \"report_fraud\": 98,\n",
              "    \"report_lost_card\": 99,\n",
              "    \"reset_settings\": 100,\n",
              "    \"restaurant_reservation\": 101,\n",
              "    \"restaurant_reviews\": 102,\n",
              "    \"restaurant_suggestion\": 103,\n",
              "    \"rewards_balance\": 104,\n",
              "    \"roll_dice\": 105,\n",
              "    \"rollover_401k\": 106,\n",
              "    \"routing\": 107,\n",
              "    \"schedule_maintenance\": 108,\n",
              "    \"schedule_meeting\": 109,\n",
              "    \"share_location\": 110,\n",
              "    \"shopping_list\": 111,\n",
              "    \"shopping_list_update\": 112,\n",
              "    \"smart_home\": 113,\n",
              "    \"spelling\": 114,\n",
              "    \"spending_history\": 115,\n",
              "    \"sync_device\": 116,\n",
              "    \"taxes\": 117,\n",
              "    \"tell_joke\": 118,\n",
              "    \"text\": 119,\n",
              "    \"thank_you\": 120,\n",
              "    \"time\": 121,\n",
              "    \"timer\": 122,\n",
              "    \"timezone\": 123,\n",
              "    \"tire_change\": 124,\n",
              "    \"tire_pressure\": 125,\n",
              "    \"todo_list\": 126,\n",
              "    \"todo_list_update\": 127,\n",
              "    \"traffic\": 128,\n",
              "    \"transactions\": 129,\n",
              "    \"transfer\": 130,\n",
              "    \"translate\": 131,\n",
              "    \"travel_alert\": 132,\n",
              "    \"travel_notification\": 133,\n",
              "    \"travel_suggestion\": 134,\n",
              "    \"uber\": 135,\n",
              "    \"update_playlist\": 136,\n",
              "    \"user_name\": 137,\n",
              "    \"vaccines\": 138,\n",
              "    \"w2\": 139,\n",
              "    \"weather\": 140,\n",
              "    \"what_are_your_hobbies\": 141,\n",
              "    \"what_can_i_ask_you\": 142,\n",
              "    \"what_is_your_name\": 143,\n",
              "    \"what_song\": 144,\n",
              "    \"where_are_you_from\": 145,\n",
              "    \"whisper_mode\": 146,\n",
              "    \"who_do_you_work_for\": 147,\n",
              "    \"who_made_you\": 148,\n",
              "    \"yes\": 149\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"problem_type\": \"single_label_classification\",\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.16.2\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]['hidden_states'][-1][:,0].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr9obGbSRzBp",
        "outputId": "cbdeeb23-6c09-4853-f3a0-87b3e3f49faf"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_states = torch.zeros(len(datasets_hf['train']), model.config.hidden_size)\n",
        "for idx, output in zip(range(0, len(datasets_hf['train']), batch_size), outputs):\n",
        "  latent_states[idx:idx+batch_size] = output['hidden_states'][-1][:,0]"
      ],
      "metadata": {
        "id": "vcAXjgAfR-7Z"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_states.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrD34CXKTnAk",
        "outputId": "a9431bb5-e8dc-4066-880c-c15e8576f0b5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IMFOt0SUUP9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}